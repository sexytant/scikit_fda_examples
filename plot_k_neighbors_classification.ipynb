{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# K-nearest neighbors classification\n\nShows the usage of the k-nearest neighbors classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Pablo Marcos Manch\u00f3n\n# License: MIT\n\nimport skfda\nfrom skfda.ml.classification import KNeighborsClassifier\n\nfrom sklearn.model_selection import (train_test_split, GridSearchCV,\n                                     StratifiedShuffleSplit)\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are going to show the usage of the K-nearest neighbors\nclassifier in their functional version, which is a extension of the\nmultivariate one, but using functional metrics between the observations.\n\nFirstly, we are going to fetch a functional dataset, such as the Berkeley\nGrowth Study. This dataset contains the height of several boys and girls\nmeasured until the 18 years of age.\nWe will try to predict the sex by using its growth curves.\n\nThe following figure shows the growth curves grouped by sex.\n\nLoads dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skfda.datasets.fetch_growth(return_X_y=True, as_frame=True)\nX = X.iloc[:, 0].values\ny = y.values\n\n# Plot samples grouped by sex\nX.plot(group=y.codes, group_names=y.categories)\n\ny = y.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the class labels are stored in an array with 0's in the male\nsamples and 1's in the positions with female ones.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can split the dataset using the sklearn function\n:func:`~sklearn.model_selection.train_test_split`.\n\nThe function will return two\n:class:`~skfda.representation.grid.FDataGrid`'s, ``X_train`` and ``X_test``\nwith the corresponding partitions, and arrays with their class labels.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n                                                    stratify=y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will fit the classifier\n:class:`~skfda.ml.classification.KNeighborsClassifier`\nwith the training partition. This classifier works exactly like the sklearn\nmultivariate classifier\n:class:`~sklearn.neighbors.KNeighborsClassifier`, but\nwill accept as input a :class:`~skfda.representation.grid.FDataGrid` with\nfunctional observations instead of an array with multivariate data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once it is fitted, we can predict labels for the test samples.\n\nTo predict the label of a test sample, the classifier will calculate the\nk-nearest neighbors and will asign the majority class. By default, it is\nused the $\\mathbb{L}^2$ distance between functions, to determine the\nneighbourhood of a sample, with 5 neighbors.\nCan be used any of the functional metrics described in\n:doc:`/modules/misc/metrics`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = knn.predict(X_test)\nprint(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :func:`~skfda.ml.classification.KNeighborsClassifier.score` method\nallows us to calculate the mean accuracy for the test data. In this case we\nobtained around 96% of accuracy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = knn.score(X_test, y_test)\nprint(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also estimate the probability of membership to the predicted class\nusing :func:`~skfda.ml.classification.KNeighborsClassifier.predict_proba`,\nwhich will return an array with the probabilities of the classes, in\nlexicographic order, for each test sample.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probs = knn.predict_proba(X_test[:5])  # Predict first 5 samples\nprint(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the sklearn\n:class:`~sklearn.model_selection.GridSearchCV` to perform a\ngrid search to select the best hyperparams, using cross-validation.\n\nIn this case, we will vary the number of neighbors between 1 and 11.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Only odd numbers, to prevent ties\nparam_grid = {'n_neighbors': np.arange(1, 12, 2)}\n\n\nknn = KNeighborsClassifier()\n\n# Perform grid search with cross-validation\nss = StratifiedShuffleSplit(n_splits=5, test_size=.25, random_state=0)\ngscv = GridSearchCV(knn, param_grid, cv=ss)\ngscv.fit(X, y)\n\n\nprint(\"Best params:\", gscv.best_params_)\nprint(\"Best score:\", gscv.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have obtained the greatest mean accuracy using 11 neighbors. The\nfollowing figure shows the score depending on the number of neighbors.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.bar(param_grid['n_neighbors'], gscv.cv_results_['mean_test_score'])\nax.set_xticks(param_grid['n_neighbors'])\nax.set_ylabel(\"Number of Neighbors\")\nax.set_xlabel(\"Test score\")\nax.set_ylim((0.9, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the functional data have been sampled in an equispaced way, or\napproximately equispaced, it is possible to use the scikit-learn vector\nmetrics with similar results.\n\nFor example, in the case of the $\\mathbb{L}^2$ distance,\nif the integral of the distance it is approximated as a\nRiemann sum, we obtain that it is proportional to the euclidean\ndistance between vectors.\n\n\\begin{align}\\|f - g \\|_{\\mathbb{L}^2} =  \\left ( \\int_a^b |f(x) - g(x)|^2 dx \\right )\n  ^{\\frac{1}{2}} \\approx \\left ( \\sum_{n=0}^{N}\\bigtriangleup h \\,|f(x_n)\n   - g(x_n)|^2 \\right ) ^ {\\frac{1}{2}}\\\\\n  = \\sqrt{\\bigtriangleup h} \\, d_{euclidean}(\\vec{f}, \\vec{g})\\end{align}\n\n\nSo, in this case, it is roughly equivalent to use this metric instead of the\nfunctional one, due to the constant multiplication not affecting the\norder of the neighbors.\n\nSetting the parameter ``sklearn_metric`` of the classifier to ``True``,\na vectorial metric of sklearn can be passed. In\n:class:`~sklearn.neighbors.DistanceMetric` there are listed all the metrics\nsupported.\n\nWe will fit the model with the sklearn distance and search for the best\nparameter. The results can vary slightly, due to the approximation during\nthe integration, but the result should be similar.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(metric='euclidean', multivariate_metric=True)\ngscv2 = GridSearchCV(knn, param_grid, cv=ss)\ngscv2.fit(X, y)\n\nprint(\"Best params:\", gscv2.best_params_)\nprint(\"Best score:\", gscv2.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The advantage of use the sklearn metrics is the computational speed, three\norders of magnitude faster. But it is not always possible to have\nequispaced samples nor do all functional metrics have a vector equivalent\nin this way.\n\nThe mean score time depending on the metric is shown below.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Mean score time (milliseconds)\")\nprint(\"L2 distance:\", 1000 *\n      np.mean(gscv.cv_results_['mean_score_time']), \"(ms)\")\nprint(\"Euclidean distance:\", 1000 *\n      np.mean(gscv2.cv_results_['mean_score_time']), \"(ms)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This classifier can be used with multivariate funcional data, as surfaces\nor curves in $\\mathbb{R}^N$, if the metric support it too.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}