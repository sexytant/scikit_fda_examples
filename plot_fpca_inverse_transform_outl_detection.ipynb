{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Outlier detection with FPCA\n",
    "\n",
    "Example of using the inverse_transform method\n",
    "in the FPCA class to detect outlier(s) from\n",
    "the reconstruction (truncation) error.\n",
    "\n",
    "In this example, we illustrate the utility of the inverse_transform method\n",
    "of the FPCA class to perform functional outlier detection.\n",
    "Roughly speaking, an outlier is a sample\n",
    "which is not representative of the dataset\n",
    "or different enough compared to a large part of the dataset.\n",
    "The intuition is the following: if the eigenbasis,\n",
    "i.e. the q>=1 first functional principal components (FPCs), is\n",
    "sufficient to linearly approximate a clean set of\n",
    "samples, then the error between an observed sample\n",
    "and its approximation w.r.t to the first 'q' FPCs should be small.\n",
    "Thus a sample with a high reconstruction error (RE)\n",
    "is likely an outlier, in the sense that\n",
    "it is underlied by a different covariance function\n",
    "compared the training samples (nonoutliers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# Author: Cl√©ment Lejeune\n",
    "# License: MIT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from skfda.preprocessing.dim_reduction.feature_extraction import FPCA\n",
    "from skfda.misc.covariances import Exponential, Gaussian\n",
    "from skfda.datasets import make_gaussian_process\n",
    "from skfda.misc.metrics import l2_distance, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed as follows:\n",
    "- We generate a clean training dataset (not supposed to contain outliers)\n",
    "and fit an FPCA with 'q' components on it.\n",
    "- We also generate a test set containing\n",
    "both nonoutliers and outliers samples.\n",
    "- Then, we fit an FPCA(n_components=q)\n",
    "and compute the principal components scores\n",
    "of train and test samples.\n",
    "- We project back the principal components scores,\n",
    "with the inverse_transform method, to the input (training data space).\n",
    "This step can be seen as the reverse projection from the eigenspace,\n",
    "spanned by the first FPCs, to the input (functional) space.\n",
    "- Finally, we compute the relative L2-norm error between\n",
    "the observed functions and their FPCs approximation.\n",
    "We flag as outlier the samples with a reconstruction error (RE)\n",
    "higher than a quantile-based threshold.\n",
    "Hence, an outlier is thus a sample that\n",
    "exhibits a different covariance function w.r.t the training samples.\n",
    "\n",
    "The train set is generated according to a Gaussian process\n",
    "with a Gaussian (i.e. squared-exponential) covariance function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Gaussian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1823/1867561124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcov_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Gaussian' is not defined"
     ]
    }
   ],
   "source": [
    "grid_size = 5 * 10**3\n",
    "\n",
    "cov_clean = Gaussian(variance=2.0, length_scale=5.0)\n",
    "\n",
    "n_train = 10**3\n",
    "train_set = make_gaussian_process(\n",
    "    n_samples=n_train,\n",
    "    n_features=grid_size,\n",
    "    start=0.0,\n",
    "    stop=25.0,\n",
    "    cov=cov_clean,\n",
    "    random_state=20\n",
    ")\n",
    "train_set_labels = np.array(['train(nonoutliers)'] * n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is generated according to a Gaussian process\n",
    "with the same covariance function for nonoutliers (50%) and\n",
    "with an exponential covariance function for outliers (50%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_test = 50\n",
    "test_set_clean = make_gaussian_process(\n",
    "    n_samples=n_test // 2,\n",
    "    n_features=grid_size,\n",
    "    start=0.0,\n",
    "    stop=25.0,\n",
    "    cov=cov_clean,\n",
    "    random_state=20\n",
    ")  # clean test set\n",
    "test_set_clean_labels = np.array(['test(nonoutliers)'] * (n_test // 2))\n",
    "\n",
    "cov_outlier = Exponential()\n",
    "\n",
    "test_set_outlier = make_gaussian_process(\n",
    "    n_samples=n_test // 2,\n",
    "    n_features=grid_size,\n",
    "    start=0.0,\n",
    "    stop=25.0,\n",
    "    cov=cov_outlier,\n",
    "    random_state=20\n",
    ")  # test set with outliers\n",
    "test_set_outlier.sample_names = [\n",
    "    'test_outl_' + str(i) for i in range(test_set_outlier.n_samples)]\n",
    "test_set_outlier_labels = np.array(['test(outliers)'] * (n_test // 2))\n",
    "\n",
    "test_set = test_set_clean.concatenate(test_set_outlier)\n",
    "test_set_labels = np.concatenate(\n",
    "    (test_set_clean_labels, test_set_outlier_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the whole dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "whole_data = train_set.concatenate(test_set)\n",
    "whole_data_labels = np.concatenate((train_set_labels, test_set_labels))\n",
    "\n",
    "fig = whole_data.plot(\n",
    "    group=whole_data_labels,\n",
    "    group_colors={\n",
    "        'train(nonoutliers)': 'grey',\n",
    "        'test(nonoutliers)': 'red',\n",
    "        'test(outliers)': 'C1'},\n",
    "    linewidth=0.95,\n",
    "    alpha=0.3,\n",
    "    legend=True\n",
    ")\n",
    "plt.title('train and test samples')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit an FPCA with an arbitrary low number of components\n",
    "compared to the input dimension (grid size).\n",
    "We compute the relative RE\n",
    "of both the training and test samples, and plot the pdf estimates.\n",
    "Errors are normalized w.r.t L2-norms of each sample\n",
    "to remove (explained) variance from the scale error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "q = 5\n",
    "fpca_clean = FPCA(n_components=q)\n",
    "fpca_clean.fit(train_set)\n",
    "train_set_hat = fpca_clean.inverse_transform(\n",
    "    fpca_clean.transform(train_set)\n",
    ")\n",
    "\n",
    "err_train = l2_distance(\n",
    "    train_set,\n",
    "    train_set_hat\n",
    ") / l2_norm(train_set)\n",
    "\n",
    "test_set_hat = fpca_clean.inverse_transform(\n",
    "    fpca_clean.transform(test_set)\n",
    ")\n",
    "err_test = l2_distance(\n",
    "    test_set,\n",
    "    test_set_hat\n",
    ") / l2_norm(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the density of the REs,\n",
    "both unconditionaly (grey and blue) and conditionaly (orange and red),\n",
    "to the rule if error >= threshold then it is an outlier.\n",
    "The threshold is computed from RE of the training samples as\n",
    "the quantile of probability 0.99.\n",
    "In other words, a sample whose RE is higher than the threshold is unlikely\n",
    "approximated as a training sample, with probability 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_density = np.linspace(0., 1.6, num=10**3)\n",
    "density_train_err = gaussian_kde(err_train)\n",
    "density_test_err = gaussian_kde(err_test)\n",
    "err_thresh = np.quantile(err_train, 0.99)\n",
    "\n",
    "density_test_err_outl = gaussian_kde(err_test[err_test >= err_thresh])\n",
    "density_test_err_inli = gaussian_kde(err_test[err_test < err_thresh])\n",
    "\n",
    "# density estimate of train errors\n",
    "plt.plot(\n",
    "    x_density,\n",
    "    density_train_err(x_density),\n",
    "    label='Error train',\n",
    "    color='grey'\n",
    ")\n",
    "\n",
    "# density estimate of test errors\n",
    "plt.plot(\n",
    "    x_density,\n",
    "    density_test_err(x_density),\n",
    "    label='Error test (outliers+nonoutliers)',\n",
    "    color='C0'\n",
    ")\n",
    "\n",
    "# outlyingness threshold\n",
    "plt.vlines(\n",
    "    err_thresh,\n",
    "    ymax=max(density_train_err(x_density)),\n",
    "    ymin=0.0,\n",
    "    label='thresh=quantile(p=0.99)',\n",
    "    linestyles='dashed',\n",
    "    color='black'\n",
    ")\n",
    "\n",
    "# density estimate of the error of test samples flagged as outliers\n",
    "plt.plot(\n",
    "    x_density,\n",
    "    density_test_err_outl(x_density),\n",
    "    label='Error test>= thresh (outliers)',\n",
    "    color='C1'\n",
    ")\n",
    "\n",
    "# density estimate of the error of test samples flagged as nonoutliers\n",
    "plt.plot(\n",
    "    x_density,\n",
    "    density_test_err_inli(x_density),\n",
    "    label='Error test< thresh (nonoutliers)',\n",
    "    color='red'\n",
    ")\n",
    "\n",
    "plt.xlabel('Relative L2-norm reconstruction errors')\n",
    "plt.ylabel('Density (unnormalized)')\n",
    "plt.title(f'Densities of reconstruction errors with {q} components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the outliers are all detected with this method,\n",
    "with no false positive (wrongly) in the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Flagged outliers: ')\n",
    "print(test_set_labels[err_test >= err_thresh])\n",
    "print('Flagged nonoutliers: ')\n",
    "print(test_set_labels[err_test < err_thresh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distribution of the training samples (grey) REs\n",
    "is unimodal and quite skewed toward 0. This means that\n",
    "the training samples are well recovered with 5 FPCs if we allow\n",
    "an reconsutrction error rate around 0.4.\n",
    "On the contrary, the distribution of the\n",
    "test samples (blue) REs is bimodal,\n",
    "where the two modes seem to be similar,\n",
    "meaning that half of the test samples is consistently approximated w.r.t\n",
    "training samples and the other half is poorly approximated in the FPCs basis.\n",
    "\n",
    "The distribution underlying the left blue mode (red) is the one of\n",
    "test samples REs flagged as nonoutliers, i.e. having a RE_i<threshold.\n",
    "This means that test samples whose RE is low are effectively nonoutliers.\n",
    "Conversely, the distribution of REs underlying the right blue mode (orange)\n",
    "is the one of the test samples REs that we flagged as outliers.\n",
    "\n",
    "To conclude this empirical example, the inverse_transform of FPCA can be used\n",
    "to detect outliers based on the magnitude of the REs\n",
    "compared to the training samples.\n",
    "Note that, here, an outlier is implicitly discriminated\n",
    "according to its covariance function.\n",
    "If a sample has a similar covariance function,\n",
    "compared to those of the training samples,\n",
    "it is very unlikely an outlier.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
